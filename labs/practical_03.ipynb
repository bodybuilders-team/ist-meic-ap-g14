{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Logistic Regression "
   ],
   "id": "77679c0eca61d6e7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T17:30:23.299590600Z",
     "start_time": "2023-11-26T17:30:22.402258200Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "id": "1047d9a2df398f8e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Consider the following training data:\n",
    "\n",
    "$\\mathbf{x^{(1)}} =\\begin{bmatrix} 1 \\\\ 1  \\\\ \\end{bmatrix} $,  $\\mathbf{x^{(2)}} =\\begin{bmatrix} 2 \\\\ 1  \\\\ \\end{bmatrix} $,   $\\mathbf{x^{(3)}} =\\begin{bmatrix} 1 \\\\ 3 \\\\ \\end{bmatrix} $,      $\\mathbf{x^{(4)}} =\\begin{bmatrix} 3 \\\\ 3  \\\\ \\end{bmatrix} $\n",
    "\n",
    "$y^{(1)} = 1.4$, $y^{(2)} = 0.5$, $y^{(3)} = 2$, $y^{(4)} = 2.5$\n",
    "\n",
    "Our goal is to fit a Linear Regression that minimizes the sum of squared errors on the training data.\n",
    "\n",
    "❓ Find the closed form solution for a linear regression that minimizes the sum of squared errors on the training data. You need to:\n",
    " 1. Define matrix $\\mathbf{X}$ with shape $n \\times (d+1)$, accounting for the bias parameter.\n",
    " 2. Define target vector $\\mathbf{y}$\n",
    " 3. Use closed form solution to get optimal $\\mathbf{\\hat{w}}$\n",
    "\n",
    "Useful functions: [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)"
   ],
   "id": "a075a46841cc58df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find w - your code here\n",
    "X = np.array([[1, 1], [2, 1], [1, 3], [3, 3]])\n",
    "y = np.array([1.4, 0.5, 2, 2.5])\n",
    "\n",
    "# Add column with ones to handle the bias coefficient.\n",
    "X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "\n",
    "# Compute weights: closed form solution: w = (X^T X)^-1 X^T y\n",
    "w_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(w_hat)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-26T17:30:30.211834200Z"
    }
   },
   "id": "dfac616fba9f86bb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Predict the target value for $\\mathbf{x_{query}} = [2, 3]^\\top$"
   ],
   "id": "5a1233cae99e3620"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "x_query = np.array([1, 2, 3])\n",
    "y_query = w_hat.dot(x_query)"
   ],
   "id": "569cc519a34f19fd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Use the provided function `plot_hyperplane_3d` to plot the training data and the predicted hyperplane. "
   ],
   "id": "6dca13bbd69c37a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that plots hyperplane defined by weights w, sucht that w.x = 0.\n",
    "points: matrix with observations where the fisrt column is supposed to be the bias constant column\n",
    "target: target vector  \n",
    "'''\n",
    "\n",
    "\n",
    "def plot_hyperplane_3d(w, points, target):\n",
    "    x1 = np.linspace(0, 4, 10)\n",
    "    x2 = np.linspace(0, 4, 10)\n",
    "    a, b = np.meshgrid(x1, x2)\n",
    "    N = x1.size\n",
    "    X = np.array([np.ones(N ** 2), a.ravel(), b.ravel()]).transpose()\n",
    "    o = X.dot(w)\n",
    "\n",
    "    ax = plt.figure().gca(projection='3d')\n",
    "    ax.plot_surface(a, b, np.reshape(o, (N, N)), alpha=0.5,\n",
    "                    linewidth=0, antialiased=True)\n",
    "    ax.scatter(points[:, 1], points[:, 2], y, color='red')"
   ],
   "id": "8be0e9ffc6d94ee2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your solution here\n",
    "plot_hyperplane_3d(w_hat, X, y)"
   ],
   "id": "1b3fb8aaf2558be2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the mean squared error produced by the linear regression."
   ],
   "id": "13f3e6c23c79be73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# MSE = 1/n sum_i (y_i - y_hat_i)^2\n",
    "y_hat = X.dot(w_hat)\n",
    "mse = np.mean((y - y_hat) ** 2)\n",
    "print(mse)"
   ],
   "id": "411788588cd63859"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Consider the following training data:\n",
    "\n",
    "$\\mathbf{x^{(1)}} =\\begin{bmatrix} 3 \\end{bmatrix} $,  $\\mathbf{x^{(2)}} =\\begin{bmatrix} 4 \\end{bmatrix} $,   $\\mathbf{x^{(3)}} =\\begin{bmatrix} 6 \\end{bmatrix} $,      $\\mathbf{x^{(4)}} =\\begin{bmatrix} 10 \\end{bmatrix} $, \n",
    " $\\mathbf{x^{(5)}} =\\begin{bmatrix} 12 \\end{bmatrix} $\n",
    "\n",
    "$y^{(1)} = 1.5$, $y^{(2)} = 11.3$, $y^{(3)} = 20.4$, $y^{(4)} = 35.8$, $y^{(5)} = 70.1$\n",
    "\n",
    "❓ Define `x` and `y` arrays with your data and plot it.\n",
    "\n",
    "*Hint:* Think about what shape your data should have. E.g.:`x.shape` should be `(5,1)`"
   ],
   "id": "6e9902353eb5a607"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "x = np.array([[3], [4], [6], [10], [12]])\n",
    "y = np.array([1.5, 11.3, 20.4, 35.8, 70.1])\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ],
   "id": "e5052fc6f5c18687"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to consider non-linear regressions. In this question you will adopt **feature transformations** in order to fit non-linear regressions on the training data.\n",
    "\n",
    "❓ Apply a logarithmic transformation $\\phi(x_1) = log(x_1)$ and write down the closed form solution for this non-linear regression that minimizes the sum of squared errors on the training data. \n",
    "\n",
    "❓ Complete function `non_linear_regression` below:"
   ],
   "id": "7e7ee33f2f646d2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "    x - vector of 1D observations, should have shape (n x 1)\n",
    "    y - target vector, should have shape (n,)\n",
    "    phi - function (feature transformation to apply)\n",
    "\n",
    "Output: \n",
    "    X_phi - feature matrix of shape (n x 2) (accounting for bias)\n",
    "    w - vector of weights found by the Least Squares Method\n",
    "'''\n",
    "\n",
    "\n",
    "def non_linear_regression(x, y, phi):\n",
    "    # Apply transformation\n",
    "    x_phi = phi(x)\n",
    "    # Construct X vector (accounting for bias)\n",
    "    X_phi = np.concatenate([np.ones((x_phi.shape[0], 1)), x_phi], axis=1)\n",
    "    # Find solution: w = (X^T X)^-1 X^T y\n",
    "    w = np.linalg.inv(X_phi.T.dot(X_phi)).dot(X_phi.T).dot(y)\n",
    "\n",
    "    return X_phi, w\n"
   ],
   "id": "4a39c286472e5b93"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Fit a regression on the training data with transformation $\\phi(x_1) = log(x_1)$.\n",
    "\n",
    "❓ Fit a regression on the training data with transformation $\\phi(x_1) = x_1^2$."
   ],
   "id": "a6e5898fc94efb7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi1(x):\n",
    "    return np.log(x)\n",
    "\n",
    "\n",
    "def phi2(x):\n",
    "    return x ** 2"
   ],
   "id": "9c69f22deb53c884"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_phi1, w1 = non_linear_regression(x, y, phi1)\n",
    "X_phi2, w2 = non_linear_regression(x, y, phi2)"
   ],
   "id": "a78192a6613c8128"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Plot both regressions along with the training points."
   ],
   "id": "a318e3d912d22865"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "# your regressions here \n",
    "x1 = np.linspace(0, 15, 100)\n",
    "plt.plot(x1, w1[0] + w1[1] * np.log(x1), 'r')\n",
    "plt.plot(x1, w2[0] + w2[1] * x1 ** 2, 'g')\n",
    "\n",
    "plt.show()"
   ],
   "id": "8f87c4464d5f11e9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Which one is a better fit? "
   ],
   "id": "eda1ce87856b9e76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ],
   "id": "227d3519a168efd3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **(Optional):** Generalize your function to be able to apply several feature transformations, *i.e.*, to perform regressions of type:\n",
    "\n",
    "$$\\hat{y}(x) = w_0 + w_1\\phi_1(x) + ... + w_d\\phi_d(x)$$\n",
    "\n",
    "Use your function to fit a polynomial of degree 3 to the training data."
   ],
   "id": "53a78b2124b73c0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_non_linear_regression(x, y, phi_list):\n",
    "    X_phi = np.concatenate([np.ones((x.shape[0], 1))] + [phi(x) for phi in phi_list], axis=1)  # (n x (d+1))\n",
    "\n",
    "    w = np.linalg.inv(X_phi.T.dot(X_phi)).dot(X_phi.T).dot(y)  # (d+1 x 1)\n",
    "\n",
    "    return X_phi, w"
   ],
   "id": "4887d44254eae184"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "In this exercise, we will consider binary logistic regression:\n",
    "\n",
    "\n",
    "$$p_{\\mathbf{w}}\\left(y=1 \\mid \\mathbf{x}\\right) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) = \\frac{1}{1+\\exp\\left(-\\mathbf{w}\\cdot\\mathbf{x}\\right)}$$\n",
    "\n",
    "with the following training data:\n",
    "\n",
    "\n",
    "$\\mathbf{x^{(1)}} =\\begin{bmatrix} -1 \\\\ 0  \\\\ \\end{bmatrix} $,  $\\mathbf{x^{(2)}} =\\begin{bmatrix} 0 \\\\ 0.25  \\\\ \\end{bmatrix} $,   $\\mathbf{x^{(3)}} =\\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix} $,      $\\mathbf{x^{(4)}} =\\begin{bmatrix} 1 \\\\ -1  \\\\ \\end{bmatrix} $\n",
    "\n",
    "$y^{(1)} = 0$, $y^{(2)} = 1$, $y^{(3)} = 1$, $y^{(4)} = 0$\n",
    "\n",
    "using the cross-entropy loss function.\n",
    "\n",
    "❓ Complete function `lr_batch_gd` that computes one epoch of batch gradient descent for the logistic regression with cross-entropy loss:"
   ],
   "id": "fea0f740c9ecb0aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Receives:\n",
    "    inputs: observations array of shape (n x (p+1)) accounting for bias\n",
    "    labels: array of target values of shape (n, )\n",
    "    w: initial array of weights shape ((p+1), )\n",
    "    eta: scalar for learning rate\n",
    "Returns:\n",
    "    w: updated weights\n",
    "'''\n",
    "\n",
    "\n",
    "def lr_batch_gd(inputs, labels, w, eta):\n",
    "    # Compute vector of probabilities of size p+1.\n",
    "    p = 1 / (1 + np.exp(-inputs.dot(w)))\n",
    "\n",
    "    # Compute gradient of loss function; vector of size p+1.\n",
    "    grad = -inputs.T.dot(labels - p)\n",
    "\n",
    "    # Gradient descent update of w\n",
    "    w = w - eta * grad\n",
    "\n",
    "    return w"
   ],
   "id": "c052539054db67f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Test your function on the provided training data assuming an initialization of all zeros for the weights and a learning rate of $\\eta=1$"
   ],
   "id": "fafe0cf85b3a71fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-1, 0],\n",
    "              [0, 0.25],\n",
    "              [1, 1],\n",
    "              [1, -1]])\n",
    "\n",
    "# Adding column of 1s to account for bias\n",
    "inputs = np.concatenate([np.ones((x.shape[0], 1)), x], axis=1)\n",
    "\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize weights and set learning rate\n",
    "w = np.zeros(inputs.shape[1])\n",
    "eta = 1\n",
    "\n",
    "# Test function\n",
    "w = lr_batch_gd(inputs, y, w, eta)\n",
    "print(w)"
   ],
   "id": "daf60576b6139dc1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use **stochastic gradient descent** where we make one weight update for each training example, i.e. in one epoch there should be $n$ weight updates.\n",
    "\n",
    "❓ Complete function `lr_sgd` to execute one epoch of stochastic gradient descent. Test your function on the training data.\n",
    "\n",
    "*Hint:* Python's [`zip`](https://www.geeksforgeeks.org/zip-in-python/) function is useful to iterate through several lists/arrays in parallel. "
   ],
   "id": "e6dc1e078e65b4ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Receives:\n",
    "    inputs: observations array of shape (n x (p+1)) accounting for bias\n",
    "    labels: array of target values of shape (n, )\n",
    "    w: initial array of weights shape ((p+1), )\n",
    "    eta: scalar for learning rate\n",
    "Returns:\n",
    "    w: updated weights\n",
    "'''\n",
    "\n",
    "\n",
    "def lr_sgd(inputs, labels, w, eta):\n",
    "    # For each training example \n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Compute probability y_hat\n",
    "        y_hat = 1 / (1 + np.exp(-x.dot(w)))\n",
    "        # Compute gradient\n",
    "        grad = -(y - y_hat) * x\n",
    "        # Update weights:\n",
    "        w = w - eta * grad\n",
    "\n",
    "    return w"
   ],
   "id": "b9d89a059915cf4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute 3 epochs of SGD on your training data and use the provided `plot_separation_line` to see your end results."
   ],
   "id": "3b401dfbe3c9d95f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot separation line associated with model w, along with the data.\n",
    "def plot_separation_line(inputs, labels, w):\n",
    "    # Plot data.\n",
    "    plt.plot(inputs[labels == 0, 1], inputs[labels == 0, 2], \"b.\")\n",
    "    plt.plot(inputs[labels == 1, 1], inputs[labels == 1, 2], \"r.\")\n",
    "\n",
    "    # Plot model separation line.\n",
    "    # w0 + w1*x1 + w2*x2 = 0.\n",
    "    x1 = np.array([-2, 2])\n",
    "    x2 = (-w[0] - w[1] * x1) / w[2]\n",
    "    plt.plot(x1, x2, 'k--')\n",
    "    plt.show()"
   ],
   "id": "f85d0a38289f158"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 epochs\n",
    "for i in range(3):\n",
    "    w = lr_sgd(inputs, y, w, eta)\n",
    "    plot_separation_line(inputs, y, w)\n"
   ],
   "id": "1e8314f1ede69f6f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Logistic Regression can also be used for multi-class classification. We will use logistic regression on real data to solve the task of classifying handwritten digits.\n",
    "\n",
    "The dataset is loaded below: "
   ],
   "id": "d05a0ad7a9e4ffbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  # num_examples x num_features\n",
    "labels = data.target  # num_examples x num_labels\n",
    "\n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))  # labels are 0, 1, ..., num_labels-1\n",
    "\n",
    "print(f'There are {n} observations with {p} features classified into {n_classes} classes.')\n",
    "\n",
    "# Augment points with a dimension for the bias.\n",
    "inputs = np.concatenate([np.ones((n, 1)), inputs], axis=1)\n",
    "\n",
    "# Observation example\n",
    "plt.matshow(data.images[17])\n",
    "plt.show()"
   ],
   "id": "df1f4e24429c17ba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `multi_class_lr_epoch` to run one epoch of stochastic gradient descent for multi-class logistic regression.\n",
    "\n",
    "*Hint:* When dealing with matrix calculus one needs to be careful with dimensions. An array with shape `(n,)` is not the same as an array with shape `(n,1)`. Function [`np.expand_dims`](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html) is one way to reshape your arrays into the desired format."
   ],
   "id": "fa64df542ae47587"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Receives:\n",
    "    inputs: observations array of shape (n x (p+1)) accounting for bias\n",
    "    labels: array of target values of shape (n, )\n",
    "    W: initial array of weights of shape (n_classes x (p+1) )\n",
    "    eta: scalar for learning rate\n",
    "Returns:\n",
    "    w: updated weights\n",
    "'''\n",
    "\n",
    "\n",
    "def multi_class_lr_epoch(inputs, labels, W, eta):\n",
    "    # For each observation in data\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Compute probability scores according to the model (num_labels x 1).\n",
    "        scores = W.dot(x)\n",
    "\n",
    "        # One-hot encode true label (num_labels x 1).\n",
    "        y_one_hot = np.zeros(n_classes)\n",
    "        y_one_hot[y] = 1\n",
    "\n",
    "        # Softmax function\n",
    "        # This gives the label probabilities according to the model (num_labels x 1).\n",
    "        probs = np.exp(scores) / np.sum(np.exp(scores))\n",
    "\n",
    "        # SGD update. W is num_labels x num_features.\n",
    "        W = W - eta * np.outer(probs - y_one_hot, x)\n",
    "    \n",
    "    return W"
   ],
   "id": "4512d9ae61954bd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into train an test sets:"
   ],
   "id": "63baa38d7b2b4dd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)"
   ],
   "id": "c16e5c6eecf9443e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Run 100 epochs of your multi-class Logistic Regression algorithm on the training data, initializing weight matrix with zeros and learning rate of 0.001."
   ],
   "id": "6d7ec7614ef9fc4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "W = np.zeros((n_classes, p + 1))\n",
    "for i in range(100):\n",
    "    W = multi_class_lr_epoch(X_train, y_train, W, 0.001)"
   ],
   "id": "904d51170ba65820"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `multi_class_classify` to generate predicted labels for provided inputs and trained weights $W$."
   ],
   "id": "8ea0898f507cee24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Receives:\n",
    "    inputs: observations of shape (n x (p+1))\n",
    "    W: weight matrix of shape (n_classes x p+1)\n",
    "Outputs: \n",
    "    y_hat: array of predicted labels of shape (n,)\n",
    "'''\n",
    "\n",
    "\n",
    "def multi_class_classify(inputs, W):\n",
    "    # Complete function\n",
    "    scores = inputs.dot(W.T)\n",
    "    y_hat = np.argmax(scores, axis=1)\n",
    "\n",
    "    return y_hat"
   ],
   "id": "564ebc8ce0332e6a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Use your function to obtain predictions for both train and test sets and compute their accuracy scores."
   ],
   "id": "a094372a2ec8e8ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "y_hat_train = multi_class_classify(X_train, W)\n",
    "y_hat_test = multi_class_classify(X_test, W)"
   ],
   "id": "c12237a7d4edbc4d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with the ones obtained with Sklearn's implementation of Logistic Regression:"
   ],
   "id": "1c7d6c128b8a4d21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=False, penalty='none')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ],
   "id": "44a33761d96e03dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
